{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b8724e65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8724e65",
        "outputId": "a10eea08-56f0-4c9d-c97a-a50f68e49f6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import argparse\n",
        "import re\n",
        "\n",
        "# Third-party libraries\n",
        "import contractions\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump, load\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# NLTK downloads (optional to keep here, can also be run separately)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bIYF332EU8zt",
      "metadata": {
        "id": "bIYF332EU8zt"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "VLJfiPFfU42E",
      "metadata": {
        "id": "VLJfiPFfU42E"
      },
      "outputs": [],
      "source": [
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath, sep='\\t', names=['title', 'from', 'genre', 'director', 'plot'])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "J7nx_vn7VDpN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J7nx_vn7VDpN",
        "outputId": "40d18901-8e95-47e0-e161-49ed786f2537"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>from</th>\n",
              "      <th>genre</th>\n",
              "      <th>director</th>\n",
              "      <th>plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ela Cheppanu</td>\n",
              "      <td>Telugu</td>\n",
              "      <td>romance</td>\n",
              "      <td>Ramana</td>\n",
              "      <td>Sekhar (Tarun) is a graduate from IIM and work...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Nightmare on Elm Street</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>Samuel Bayer</td>\n",
              "      <td>Kris Fowles (Katie Cassidy) goes to the Spring...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>American Gothic</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>John Hough</td>\n",
              "      <td>Cynthia is traumatized by the death of her bab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gang</td>\n",
              "      <td>Bollywood</td>\n",
              "      <td>crime</td>\n",
              "      <td>Mazhar Khan</td>\n",
              "      <td>Four friends, Gangu (Jackie Shroff), Abdul (Na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Intimate Relations</td>\n",
              "      <td>British</td>\n",
              "      <td>drama</td>\n",
              "      <td>Charles Frank</td>\n",
              "      <td>Crisis in a middle-class family when the son f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       title       from    genre       director  \\\n",
              "0               Ela Cheppanu     Telugu  romance         Ramana   \n",
              "1  A Nightmare on Elm Street   American   horror   Samuel Bayer   \n",
              "2            American Gothic   American   horror     John Hough   \n",
              "3                       Gang  Bollywood    crime    Mazhar Khan   \n",
              "4         Intimate Relations    British    drama  Charles Frank   \n",
              "\n",
              "                                                plot  \n",
              "0  Sekhar (Tarun) is a graduate from IIM and work...  \n",
              "1  Kris Fowles (Katie Cassidy) goes to the Spring...  \n",
              "2  Cynthia is traumatized by the death of her bab...  \n",
              "3  Four friends, Gangu (Jackie Shroff), Abdul (Na...  \n",
              "4  Crisis in a middle-class family when the son f...  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_data('train.txt')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb7de66",
      "metadata": {
        "id": "3cb7de66"
      },
      "source": [
        "# Clean the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d6d2e2e1",
      "metadata": {
        "id": "d6d2e2e1"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Check if the text is a non-empty string\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "\n",
        "    # Skip applying contractions if the text is too long or complex\n",
        "    if len(text) > 500:  # Threshold to skip contraction expansion for long texts\n",
        "        return text\n",
        "\n",
        "    # Try expanding contractions safely\n",
        "    try:\n",
        "        text = contractions.fix(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error expanding contractions: {e}, for text: {text[:100]}...\")  # Only print the first 100 characters\n",
        "        return text  # Return the original text if expansion fails\n",
        "\n",
        "    # Remove c\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0c444d",
      "metadata": {
        "id": "6c0c444d"
      },
      "source": [
        "# Function to convert nltk POS tag to wordnet POS tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "450a9c24",
      "metadata": {
        "id": "450a9c24"
      },
      "outputs": [],
      "source": [
        "# Function to convert nltk POS tag to wordnet POS tag\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ  # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB  # Verb\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN  # Noun\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV  # Adverb\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663b118d",
      "metadata": {
        "id": "663b118d"
      },
      "source": [
        "# Lemmatize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9e6b0e8c",
      "metadata": {
        "id": "9e6b0e8c"
      },
      "outputs": [],
      "source": [
        "# Function to lemmatize the text\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Clean the text\n",
        "    text = clean_text(text)\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Remove stop words and non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha() and word]\n",
        "    # Perform POS tagging\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    # Lemmatize each token using the POS tag\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "999ea6fa",
      "metadata": {},
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8/nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munhappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[36], line 7\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Remove stop words and non-alphabetic tokens\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m word]\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8/nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "lemmatize_text(\"unhappy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18de5503",
      "metadata": {
        "id": "18de5503"
      },
      "source": [
        "# Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8f742ce4",
      "metadata": {
        "id": "8f742ce4"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(data, lemmatize, combine_fields):\n",
        "    # Extract relevant columns (plot, combine_fields, genre)\n",
        "    selected_fields = ['plot'] + combine_fields\n",
        "    if 'genre' in data.columns:\n",
        "        selected_fields.append('genre')\n",
        "    data = data[selected_fields].copy()\n",
        "\n",
        "    # Handle missing values: drop rows with missing 'plot', fill missing combine_fields with ''\n",
        "    data.dropna(subset=['plot'], inplace=True)  # Ensure 'plot' is not NaN\n",
        "    for field in combine_fields:\n",
        "        data[field] = data[field].fillna('')  # Replace NaN in combine_fields with empty strings\n",
        "\n",
        "    # Combine plot and other specified fields into a single feature\n",
        "    data['combined_text'] = data['plot']\n",
        "    for field in combine_fields:\n",
        "        data['combined_text'] += ' ' + data[field]\n",
        "\n",
        "    # Apply lemmatization if specified\n",
        "    if lemmatize:\n",
        "        data['combined_text'] = data['combined_text'].apply(lemmatize_text)\n",
        "    else:\n",
        "        # Clean the text anyway\n",
        "        data['combined_text'] = data['combined_text'].apply(clean_text)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "22e2fa02",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>from</th>\n",
              "      <th>genre</th>\n",
              "      <th>director</th>\n",
              "      <th>plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ela Cheppanu</td>\n",
              "      <td>Telugu</td>\n",
              "      <td>romance</td>\n",
              "      <td>Ramana</td>\n",
              "      <td>Sekhar (Tarun) is a graduate from IIM and work...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Nightmare on Elm Street</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>Samuel Bayer</td>\n",
              "      <td>Kris Fowles (Katie Cassidy) goes to the Spring...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>American Gothic</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>John Hough</td>\n",
              "      <td>Cynthia is traumatized by the death of her bab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gang</td>\n",
              "      <td>Bollywood</td>\n",
              "      <td>crime</td>\n",
              "      <td>Mazhar Khan</td>\n",
              "      <td>Four friends, Gangu (Jackie Shroff), Abdul (Na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Intimate Relations</td>\n",
              "      <td>British</td>\n",
              "      <td>drama</td>\n",
              "      <td>Charles Frank</td>\n",
              "      <td>Crisis in a middle-class family when the son f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8036</th>\n",
              "      <td>Sa 'Yo Lamang</td>\n",
              "      <td>Filipino</td>\n",
              "      <td>drama</td>\n",
              "      <td>Laurice Guillen</td>\n",
              "      <td>Dianne seems to have a happy family. She and h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8037</th>\n",
              "      <td>The Lemon Sisters</td>\n",
              "      <td>American</td>\n",
              "      <td>drama</td>\n",
              "      <td>Joyce Chopra</td>\n",
              "      <td>Three lifelong friends work the bars in 1980's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8038</th>\n",
              "      <td>Dil Ne Phir Yaad Kiya</td>\n",
              "      <td>Bollywood</td>\n",
              "      <td>romance</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>City-based Ashok, who works as a Salesman in a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8039</th>\n",
              "      <td>Rehnaa Hai Terre Dil Mein</td>\n",
              "      <td>Bollywood</td>\n",
              "      <td>romance</td>\n",
              "      <td>Gautham Menon</td>\n",
              "      <td>The story starts with narration by Maddy (R. M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8040</th>\n",
              "      <td>Things to Come</td>\n",
              "      <td>British</td>\n",
              "      <td>sci-fi</td>\n",
              "      <td>William Cameron Menzies</td>\n",
              "      <td>In the British city of Everytown, businessman ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8041 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          title       from    genre                 director  \\\n",
              "0                  Ela Cheppanu     Telugu  romance                   Ramana   \n",
              "1     A Nightmare on Elm Street   American   horror             Samuel Bayer   \n",
              "2               American Gothic   American   horror               John Hough   \n",
              "3                          Gang  Bollywood    crime              Mazhar Khan   \n",
              "4            Intimate Relations    British    drama            Charles Frank   \n",
              "...                         ...        ...      ...                      ...   \n",
              "8036              Sa 'Yo Lamang   Filipino    drama          Laurice Guillen   \n",
              "8037          The Lemon Sisters   American    drama             Joyce Chopra   \n",
              "8038      Dil Ne Phir Yaad Kiya  Bollywood  romance                  Unknown   \n",
              "8039  Rehnaa Hai Terre Dil Mein  Bollywood  romance            Gautham Menon   \n",
              "8040             Things to Come    British   sci-fi  William Cameron Menzies   \n",
              "\n",
              "                                                   plot  \n",
              "0     Sekhar (Tarun) is a graduate from IIM and work...  \n",
              "1     Kris Fowles (Katie Cassidy) goes to the Spring...  \n",
              "2     Cynthia is traumatized by the death of her bab...  \n",
              "3     Four friends, Gangu (Jackie Shroff), Abdul (Na...  \n",
              "4     Crisis in a middle-class family when the son f...  \n",
              "...                                                 ...  \n",
              "8036  Dianne seems to have a happy family. She and h...  \n",
              "8037  Three lifelong friends work the bars in 1980's...  \n",
              "8038  City-based Ashok, who works as a Salesman in a...  \n",
              "8039  The story starts with narration by Maddy (R. M...  \n",
              "8040  In the British city of Everytown, businessman ...  \n",
              "\n",
              "[8041 rows x 5 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "14sI3xeaWnro",
      "metadata": {
        "id": "14sI3xeaWnro"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8/nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m combine_fields \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_fields\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[27], line 21\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(data, lemmatize, combine_fields)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Apply lemmatization if specified\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lemmatize:\n\u001b[1;32m---> 21\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Clean the text anyway\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Remove stop words and non-alphabetic tokens\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m word]\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\tester.LAPTOP-NJPROMP8\\OneDrive - Universidade de Lisboa\\Aaa Técnico\\1. MEIC\\LN\\project\\LN-proj\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8/nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\OneDrive - Universidade de Lisboa\\\\Aaa Técnico\\\\1. MEIC\\\\LN\\\\project\\\\LN-proj\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tester.LAPTOP-NJPROMP8\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "combine_fields = ['from', 'director', 'title']\n",
        "data = preprocess_data(data, True, combine_fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecfcf52e",
      "metadata": {
        "id": "ecfcf52e"
      },
      "source": [
        "# Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c3968266",
      "metadata": {
        "id": "c3968266"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "def split_data(data):\n",
        "    X = data['combined_text']\n",
        "    y = data['genre']\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded)\n",
        "    return X_train, X_test, y_train, y_test, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "blPLZCX3n4H3",
      "metadata": {
        "id": "blPLZCX3n4H3"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, label_encoder = split_data(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8596b3",
      "metadata": {
        "id": "6f8596b3"
      },
      "source": [
        "# Feature extraction using BOW Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Ds8p9Rm8iMmO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Ds8p9Rm8iMmO",
        "outputId": "c7bd79fa-728b-4e71-c2ad-45956f9ebc15"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'max_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2b38c712f1bf>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-2b38c712f1bf>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# create the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     vectorizer = CountVectorizer(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         min_df=5)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_features' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# create the vocabulary\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizers = {\n",
        "        \"BOW:\" \n",
        "    }\n",
        "vectorizer_BOW = CountVectorizer(\n",
        "    max_features=max_features,\n",
        "    ngram_range=ngram_range,\n",
        "    min_df=5)\n",
        "\n",
        "# Feature extraction using BOW Vectorizer\n",
        "def extract_features(X_train, X_test, max_features, ngram_range):\n",
        "    # create the vocabulary\n",
        "    \n",
        "    X_train_BOW = vectorizer.fit_transform(X_train)\n",
        "    X_test_BOW = vectorizer.transform(X_test)\n",
        "    return X_train_BOW, X_test_BOW, vectorizer\n",
        "\n",
        "\n",
        "extract_features(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Su-q0Jpnk_CW",
      "metadata": {
        "id": "Su-q0Jpnk_CW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "890dcfb3",
      "metadata": {
        "id": "890dcfb3"
      },
      "source": [
        "# Train SVM with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZhUqV8chU2bA",
      "metadata": {
        "id": "ZhUqV8chU2bA"
      },
      "outputs": [],
      "source": [
        "# Grid search function for threshold optimization and SVM training\n",
        "def grid_search_and_train_svm(X_train, X_validation, y_train, y_validation, max_features, ngram_range, cv=5, scoring='accuracy'):\n",
        "    df_stop_words = stop_word_frequencies(X_train)\n",
        "\n",
        "    best_threshold = None\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    thresholds = np.arange(0.10, 1.10, 0.10)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        stop_words = filtered_stop_words(df_stop_words, threshold)\n",
        "\n",
        "        X_train_tfidf, X_validation_tfidf, vectorizer = extract_features_gs(X_train, X_validation, stop_words, max_features, ngram_range)\n",
        "\n",
        "        # Train SVM with Grid Search for hyperparameter tuning\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear'],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "        grid_search = GridSearchCV(SVC(), param_grid, cv=cv, n_jobs=-1)\n",
        "        grid_search.fit(X_train_tfidf, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        y_validation_pred = best_model.predict(X_validation_tfidf)\n",
        "        accuracy = accuracy_score(y_validation, y_validation_pred)\n",
        "\n",
        "        print(f\"Threshold: {threshold:.2f}, Validation Accuracy: {accuracy:.4f}, Best SVM Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"Best Threshold: {best_threshold:.2f}, Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    return best_model, best_threshold, vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9a58fb",
      "metadata": {
        "id": "8b9a58fb"
      },
      "outputs": [],
      "source": [
        "def train_svm(X_train_tfidf, y_train):\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear'],\n",
        "        'class_weight': ['balanced', None]\n",
        "    }\n",
        "    grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train_tfidf, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    print(f\"Best SVM Parameters: {grid_search.best_params_}\")\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9725a0f3",
      "metadata": {
        "id": "9725a0f3"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccef53b",
      "metadata": {
        "id": "bccef53b"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test_tfidf, y_test):\n",
        "    predictions = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    report = classification_report(y_test, predictions)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    return accuracy, cm, report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e63a3f9",
      "metadata": {
        "id": "0e63a3f9"
      },
      "source": [
        "# Save the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b8b855",
      "metadata": {
        "id": "92b8b855"
      },
      "outputs": [],
      "source": [
        "def save_model(model, vectorizer, label_encoder, model_path, vectorizer_path, label_encoder_path):\n",
        "    dump(model, model_path)\n",
        "    dump(vectorizer, vectorizer_path)\n",
        "    dump(label_encoder, label_encoder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3701949e",
      "metadata": {
        "id": "3701949e"
      },
      "source": [
        "# Load the model and vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343e0f65",
      "metadata": {
        "id": "343e0f65"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path, vectorizer_path, label_encoder_path):\n",
        "    model = load(model_path)\n",
        "    vectorizer = load(vectorizer_path)\n",
        "    label_encoder = load(label_encoder_path)\n",
        "    return model, vectorizer, label_encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d25897",
      "metadata": {
        "id": "54d25897"
      },
      "source": [
        "# Predict the genre for new movie plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "083e8e35",
      "metadata": {
        "id": "083e8e35"
      },
      "outputs": [],
      "source": [
        "def predict_genre(model, vectorizer, label_encoder, input_file, output_file, lemmatize, combine_fields):\n",
        "    data = read_csv(input_file, sep='\\t', names=['title', 'from', 'director', 'plot'])\n",
        "    data = preprocess_data(data, lemmatize, combine_fields)\n",
        "    plots = data['combined_text']\n",
        "    plot_vectors = vectorizer.transform(plots)\n",
        "    predicted_genres_encoded = model.predict(plot_vectors)\n",
        "    predicted_genres = label_encoder.inverse_transform(predicted_genres_encoded)\n",
        "\n",
        "    # Save the results to a file\n",
        "    data['genre'] = predicted_genres\n",
        "    data[['title', 'from', 'director', 'plot', 'genre']].to_csv(output_file, sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06bf060c",
      "metadata": {
        "id": "06bf060c"
      },
      "source": [
        "# Main function to run the program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5UA548SDNMxn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "5UA548SDNMxn",
        "outputId": "bec7d8fe-6619-4147-ca67-93673dd34293"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4c54de78-85e0-43b4-8482-617a633bb58a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plot</th>\n",
              "      <th>from</th>\n",
              "      <th>director</th>\n",
              "      <th>title</th>\n",
              "      <th>genre</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c54de78-85e0-43b4-8482-617a633bb58a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c54de78-85e0-43b4-8482-617a633bb58a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c54de78-85e0-43b4-8482-617a633bb58a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_be2ab8f0-fed4-437b-94c3-78783287ebf4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_be2ab8f0-fed4-437b-94c3-78783287ebf4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [plot, from, director, title, genre, combined_text]\n",
              "Index: []"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e65cef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e65cef8",
        "outputId": "ec73ae65-b106-425d-9293-463f82901060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            "genre\n",
            "drama        1676\n",
            "comedy       1193\n",
            "horror       1108\n",
            "action       1059\n",
            "romance       886\n",
            "western       829\n",
            "crime         541\n",
            "animation     535\n",
            "sci-fi        214\n",
            "Name: count, dtype: int64\n",
            "Threshold: 0.10, Validation Accuracy: 0.7113, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
            "Threshold: 0.20, Validation Accuracy: 0.7141, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
            "Threshold: 0.30, Validation Accuracy: 0.7169, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
            "Threshold: 0.40, Validation Accuracy: 0.7182, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
            "Threshold: 0.50, Validation Accuracy: 0.7141, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
            "Threshold: 0.60, Validation Accuracy: 0.7072, Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    train_filepath = 'train.txt'\n",
        "    #test_filepath = 'test_no_labels.txt'\n",
        "    #results_filepath = 'results.txt'\n",
        "    max_features = 10000\n",
        "    ngram_range = '1,3'\n",
        "    lemma = False\n",
        "    combine_fields = 'from,director,title'\n",
        "\n",
        "    # Parse ngram_range as a tuple\n",
        "    ngram_range = tuple(map(int, ngram_range.split(',')))\n",
        "\n",
        "    # Parse combine_fields as a list of field names\n",
        "    combine_fields = combine_fields.split(',')\n",
        "\n",
        "    \"\"\"# Convert the stop words string into a list (if not using 'english')\n",
        "    if stop_words != 'english':\n",
        "        stop_words = stop_words.split(',')\n",
        "    else:\n",
        "        stop_words = 'english'\"\"\"\n",
        "\n",
        "    # Load and preprocess the data\n",
        "    data = load_data(train_filepath)\n",
        "    data = preprocess_data(data, lemma, combine_fields)\n",
        "\n",
        "    # Analyze class distribution\n",
        "    print(\"Class distribution:\")\n",
        "    print(data['genre'].value_counts())\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test, label_encoder = split_data(data)\n",
        "\n",
        "    # Train SVM with hyperparameter tuning\n",
        "    best_svm_model, best_threshold, vectorizer = grid_search_and_train_svm(X_train, X_validation, y_train, y_validation, max_features, ngram_range, cv=5, scoring='accuracy')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pdDnyCuoNfAW",
      "metadata": {
        "id": "pdDnyCuoNfAW"
      },
      "outputs": [],
      "source": [
        "  # Evaluate model\n",
        "    svm_accuracy, svm_cm, svm_report = evaluate_model(best_svm_model, X_test_tfidf, y_test)\n",
        "    print(f\"Optimized SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(svm_report)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(best_svm_model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "    print(f\"Cross-validation Accuracy: {cv_scores.mean() * 100:.2f}% (+/- {cv_scores.std() * 100:.2f}%)\")\n",
        "\n",
        "    # Save the best model\n",
        "    save_model(best_svm_model, vectorizer, label_encoder, 'best_model.pkl', 'vectorizer.pkl', 'label_encoder.pkl')\n",
        "\n",
        "    # Predict genres for the test file and save results\n",
        "    predict_genre(best_svm_model, vectorizer, label_encoder, test_filepath, results_filepath, lemma, combine_fields)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQwWdQ39TmpD",
      "metadata": {
        "id": "jQwWdQ39TmpD"
      },
      "outputs": [],
      "source": [
        "def stop_words_frequency(data):\n",
        "  # CountVectorizer to create a document-term matrix for stop words\n",
        "  stop_words = set(stopwords.words('english'))  # English stop words\n",
        "  vectorizer = CountVectorizer(vocabulary=stop_words, binary=True)  # Count presence\n",
        "  X = vectorizer.fit_transform(data['title_plot'])  # Transform the title-plot feature\n",
        "\n",
        "  # Compute document frequencies\n",
        "  doc_freq = X.sum(axis=0) / X.shape[0]  # Fraction of documents (observations) containing each stop word\n",
        "\n",
        "  # Sort stop words by document frequency.\n",
        "  stop_word_df = pd.DataFrame({\n",
        "      'stop_word': vectorizer.get_feature_names_out(),\n",
        "      'doc_frequency': doc_freq.A1\n",
        "  }).sort_values(by='doc_frequency', ascending=False)\n",
        "\n",
        "  return stop_word_df\n",
        "\n",
        "def list_stop_words(df_stop_words, threshold):\n",
        "  # Filter stop words by a document frequency threshold\n",
        "  top_stop_words = df_stop_words[df_stop_words['doc_frequency'] >= threshold]\n",
        "  return list(top_stop_words['stop_word'].values)\n",
        "\n",
        "# Function to remove stop words\n",
        "def filter_tokens(tokens, top_stop_words):\n",
        "    return [word for word in tokens if word not in top_stop_words]\n",
        "\n",
        "def remove_stop_words(data, top_stop_words):\n",
        "  return data['words'].apply(filter_tokens, args=(top_stop_words,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mSHxDCdN1Gl2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "mSHxDCdN1Gl2",
        "outputId": "3782ef01-a32a-44e8-cdeb-0d3714826907"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'words'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'words'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-54cc55417d15>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_word_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-4320831e3e47>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(data, top_stop_words)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_stop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'words'"
          ]
        }
      ],
      "source": [
        "stop_words = list_stop_words(stop_word_df, 0.5)\n",
        "remove_stop_words(X_train, stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eMMBU2mNntqY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eMMBU2mNntqY",
        "outputId": "603496b4-5ff2-41f5-b567-25eaeeeecf2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"stop_word_df\",\n  \"rows\": 179,\n  \"fields\": [\n    {\n      \"column\": \"stop_word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 179,\n        \"samples\": [\n          \"any\",\n          \"him\",\n          \"each\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_frequency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.266427341357475,\n        \"min\": 0.0,\n        \"max\": 0.9769929113294367,\n        \"num_unique_values\": 119,\n        \"samples\": [\n          0.21241139161795797,\n          0.03444845168511379,\n          0.9256311404054222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "stop_word_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0629d79e-51d5-45cd-8053-02633dd1671c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stop_word</th>\n",
              "      <th>doc_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>the</td>\n",
              "      <td>0.976993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>to</td>\n",
              "      <td>0.948016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>and</td>\n",
              "      <td>0.942420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>be</td>\n",
              "      <td>0.940182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>of</td>\n",
              "      <td>0.925631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0629d79e-51d5-45cd-8053-02633dd1671c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0629d79e-51d5-45cd-8053-02633dd1671c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0629d79e-51d5-45cd-8053-02633dd1671c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c6b54c60-0460-4c33-b685-65e0b5010de4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6b54c60-0460-4c33-b685-65e0b5010de4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c6b54c60-0460-4c33-b685-65e0b5010de4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    stop_word  doc_frequency\n",
              "129       the       0.976993\n",
              "141        to       0.948016\n",
              "10        and       0.942420\n",
              "17         be       0.940182\n",
              "98         of       0.925631"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_word_df = stop_words_frequency(data)\n",
        "stop_word_df.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LN_project",
      "language": "python",
      "name": "ln_project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
