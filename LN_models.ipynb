{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86851af7",
      "metadata": {},
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1013,
      "id": "b8724e65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8724e65",
        "outputId": "033ebdf7-32c4-4150-c89a-52b4eb2293dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
<<<<<<< HEAD
          "execution_count": 1013,
=======
          "execution_count": 32,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standard library imports\n",
        "from re import sub\n",
        "import numpy as np\n",
        "\n",
        "# Third-party libraries\n",
        "from contractions import fix\n",
        "import nltk\n",
        "from pandas import DataFrame, read_csv, Series\n",
        "from joblib import dump, load\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import nlpaug.augmenter.word as naw\n",
<<<<<<< HEAD
        "import os\n",
=======
        "from os import makedirs\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "\n",
        "# NLTK downloads\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04151caf",
      "metadata": {},
      "source": [
        "## Define the hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1014,
=======
      "execution_count": 33,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "9fdbd40b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regular hyperparameters\n",
<<<<<<< HEAD
        "lemmatize = True\n",
=======
        "lemmatize = False\n",
        "combine_fields = ['from', 'director', 'title']\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "stop_words = set(stopwords.words('english'))\n",
        "max_features = 10000\n",
        "\n",
        "# Fine-tuning hyperparameters\n",
<<<<<<< HEAD
        "combine_fields = ['from', 'director', 'title']\n",
        "threshold = 0.15\n",
        "ngram_range = (1, 6)\n",
        "\n",
        "# File paths\n",
        "base = f'models/plot/{str(combine_fields)}_{int(threshold * 100)}_{str(ngram_range)}'\n",
        "os.makedirs(base, exist_ok=True)\n",
=======
        "combine_fields = ['from', 'director']\n",
        "threshold = 0.3                         # 0.0|0.3|0.5|1.0\n",
        "ngram_range = (1, 6)                    # (1, 5)|(1, 6)|(1, 7)\n",
        "\n",
        "# File paths\n",
        "base = f'models/{str(combine_fields)}_{int(threshold * 100)}_{str(ngram_range)}'\n",
        "makedirs(base, exist_ok=True)\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test_no_labels.txt'\n",
        "output_path = 'results.txt'\n",
        "label_encoder_path = f'{base}/label_encoder.plk'\n",
        "vectorizer_path = f'{base}/vectorizer.plk'\n",
        "svm_path = f'{base}/svm.plk'\n",
<<<<<<< HEAD
        "mnb_path = f'{base}/mnb.plk'"
=======
        "mnb_path = f'{base}/mnb.plk'\n",
        "report_path = f'{base}/report.txt'\n",
        "cm_path = f'{base}/cm.csv'"
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bIYF332EU8zt",
      "metadata": {
        "id": "bIYF332EU8zt"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1015,
=======
      "execution_count": 34,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "VLJfiPFfU42E",
      "metadata": {
        "id": "VLJfiPFfU42E"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "def load_data():\n",
        "    data = read_csv(train_path, sep='\\t', names=['title', 'from', 'genre', 'director', 'plot'])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1016,
=======
      "execution_count": 35,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "J7nx_vn7VDpN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J7nx_vn7VDpN",
        "outputId": "fb749847-7ac9-43ae-83f9-42a891eba5e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>from</th>\n",
              "      <th>genre</th>\n",
              "      <th>director</th>\n",
              "      <th>plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ela Cheppanu</td>\n",
              "      <td>Telugu</td>\n",
              "      <td>romance</td>\n",
              "      <td>Ramana</td>\n",
              "      <td>Sekhar (Tarun) is a graduate from IIM and work...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Nightmare on Elm Street</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>Samuel Bayer</td>\n",
              "      <td>Kris Fowles (Katie Cassidy) goes to the Spring...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>American Gothic</td>\n",
              "      <td>American</td>\n",
              "      <td>horror</td>\n",
              "      <td>John Hough</td>\n",
              "      <td>Cynthia is traumatized by the death of her bab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gang</td>\n",
              "      <td>Bollywood</td>\n",
              "      <td>crime</td>\n",
              "      <td>Mazhar Khan</td>\n",
              "      <td>Four friends, Gangu (Jackie Shroff), Abdul (Na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Intimate Relations</td>\n",
              "      <td>British</td>\n",
              "      <td>drama</td>\n",
              "      <td>Charles Frank</td>\n",
              "      <td>Crisis in a middle-class family when the son f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       title       from    genre       director  \\\n",
              "0               Ela Cheppanu     Telugu  romance         Ramana   \n",
              "1  A Nightmare on Elm Street   American   horror   Samuel Bayer   \n",
              "2            American Gothic   American   horror     John Hough   \n",
              "3                       Gang  Bollywood    crime    Mazhar Khan   \n",
              "4         Intimate Relations    British    drama  Charles Frank   \n",
              "\n",
              "                                                plot  \n",
              "0  Sekhar (Tarun) is a graduate from IIM and work...  \n",
              "1  Kris Fowles (Katie Cassidy) goes to the Spring...  \n",
              "2  Cynthia is traumatized by the death of her bab...  \n",
              "3  Four friends, Gangu (Jackie Shroff), Abdul (Na...  \n",
              "4  Crisis in a middle-class family when the son f...  "
            ]
          },
<<<<<<< HEAD
          "execution_count": 1016,
=======
          "execution_count": 35,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_data()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9bf5982",
      "metadata": {},
      "source": [
        "### Combine fields"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1017,
=======
      "execution_count": 36,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "1cecc147",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine fields\n",
        "def combine():\n",
        "    # Extract relevant columns (plot, combine_fields, genre)\n",
        "    selected_fields = ['plot'] + combine_fields\n",
        "    if 'genre' in data.columns:\n",
        "        selected_fields.append('genre')\n",
        "    combined_data = data[selected_fields].copy()\n",
        "    \n",
        "    # Handle missing values: drop rows with missing 'plot', fill missing combine_fields with ''\n",
        "    combined_data.dropna(subset=['plot'], inplace=True)  # Ensure 'plot' is not NaN\n",
        "    for field in combine_fields:\n",
        "        combined_data[field] = combined_data[field].fillna('')  # Replace NaN in combine_fields with empty strings\n",
        "\n",
        "    # Combine plot and other specified fields into a single feature\n",
        "    combined_data['combined_text'] = combined_data['plot']\n",
        "    for field in combine_fields:\n",
        "        combined_data['combined_text'] += ' ' + combined_data[field]\n",
        "\n",
        "    return combined_data"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1018,
=======
      "execution_count": 37,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "4e905fb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = combine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d790b14e",
      "metadata": {},
      "source": [
        "## Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1019,
      "id": "4321a460",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "def split_data():\n",
        "    X = data['combined_text']\n",
        "    y = data['genre']\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded)\n",
        "    return X_train, X_test, y_train, y_test, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1020,
      "id": "e7cb30b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, label_encoder = split_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1021,
      "id": "a75def83",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"models/plot/['from', 'director', 'title']_15_(1, 6)/label_encoder.plk\"]"
            ]
          },
          "execution_count": 1021,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save label encoder\n",
        "dump(label_encoder, label_encoder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c3e702",
      "metadata": {},
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1022,
=======
      "execution_count": 38,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "013fad26",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = DataFrame({'combined_text': X_train, 'genre': y_train})\n",
        "\n",
        "# Initialize the SynonymAugmenter\n",
        "aug = naw.SynonymAug(aug_src='wordnet', aug_max=30)\n",
        "augments_encoded = label_encoder.transform(['sci-fi', 'animation', 'crime'])\n",
        "\n",
        "# Define augmentation settings for each genre\n",
        "augmentation_config = {\n",
        "    'sci-fi': 3,    # Augment 3 times\n",
        "    'animation': 1, # Augment 1 time\n",
        "    'crime': 1,     # Augment 1 time\n",
        "}\n",
        "\n",
<<<<<<< HEAD
        "# Not augmenting the remaining genre\n",
        "for genre in np.unique(y_train):\n",
=======
        "# Not augmentening the remaining genre\n",
        "for genre in data['genre'].unique():\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "    if genre not in augmentation_config:\n",
        "        augmentation_config[genre] = 0  # Default to augmenting once\n",
        "\n",
        "# List to hold original and augmented text along with their genres\n",
        "all_texts = []\n",
        "all_genres = []\n",
        "\n",
        "# Loop through each genre and apply the augmentation as needed\n",
        "for genre, n in augmentation_config.items():\n",
        "    genre_data = data[data['genre'] == genre].copy()\n",
        "    \n",
        "    # Add original texts and genres to the lists\n",
        "    all_texts.extend(genre_data['combined_text'].tolist())\n",
        "    all_genres.extend([genre] * len(genre_data))  # Repeat the genre for the number of rows\n",
        "    \n",
        "    # Apply augmentation if n > 0\n",
        "    if n > 0:\n",
        "        augmented_texts = genre_data['combined_text'].apply(lambda x: aug.augment(x, n=n))\n",
        "        # Flatten the lists of augmented texts\n",
        "        augmented_texts = augmented_texts.explode().tolist()\n",
        "        # Add augmented texts and genres to the lists\n",
        "        all_texts.extend(augmented_texts)\n",
        "        all_genres.extend([genre] * len(augmented_texts))  # Repeat the genre for the number of augmented rows\n",
        "\n",
        "# Create a DataFrame with the text and genre columns\n",
        "data = DataFrame({'text': all_texts, 'genre': all_genres})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb7de66",
      "metadata": {
        "id": "3cb7de66"
      },
      "source": [
        "## Clean the text"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1023,
=======
      "execution_count": 39,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "d6d2e2e1",
      "metadata": {
        "id": "d6d2e2e1"
      },
      "outputs": [],
      "source": [
        "# Clean the text\n",
        "def clean_text(text):\n",
        "    # Check if the text is a non-empty string\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "    \n",
        "    # Skip applying contractions if the text is too long or complex\n",
        "    if len(text) > 500:  # Threshold to skip contraction expansion for long texts\n",
        "        return text\n",
        "    \n",
        "    # Try expanding contractions safely\n",
        "    try:\n",
        "        text = fix(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error expanding contractions: {e}, for text: {text[:100]}...\")  # Only print the first 100 characters\n",
        "        return text  # Return the original text if expansion fails\n",
        "    \n",
        "    # Remove special characters and digits\n",
        "    text = sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0c444d",
      "metadata": {
        "id": "6c0c444d"
      },
      "source": [
        "## Function to convert nltk POS tag to wordnet POS tag"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1024,
=======
      "execution_count": 40,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "450a9c24",
      "metadata": {
        "id": "450a9c24"
      },
      "outputs": [],
      "source": [
        "# Function to convert nltk POS tag to wordnet POS tag\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ  # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB  # Verb\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN  # Noun\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV  # Adverb\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663b118d",
      "metadata": {
        "id": "663b118d"
      },
      "source": [
        "## Lemmatize the text"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1025,
=======
      "execution_count": 41,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "9e6b0e8c",
      "metadata": {
        "id": "9e6b0e8c"
      },
      "outputs": [],
      "source": [
        "# Function to lemmatize text\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Perform POS tagging\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    # Lemmatize each token using the POS tag\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
        "    return ' '.join(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18de5503",
      "metadata": {
        "id": "18de5503"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1026,
=======
      "execution_count": 42,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "8f742ce4",
      "metadata": {
        "id": "8f742ce4"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data without stop words\n",
        "def preprocess_data():\n",
        "    # Apply lemmatization if specified\n",
        "    if lemmatize:\n",
        "        final_df['text'] = final_df['text'].apply(lemmatize_text)\n",
        "    else:\n",
        "        # Clean the text anyway\n",
        "        final_df['text'] = final_df['text'].apply(clean_text)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1027,
=======
      "execution_count": 43,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "440ebf70",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The protagonist, space archaeologist Simon Wat...</td>\n",
              "      <td>sci-fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A woman (Lanell Cado) steps out of a shower an...</td>\n",
              "      <td>sci-fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The plot involves rogue elements of the commun...</td>\n",
              "      <td>sci-fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Set in contemporary England, the film follows ...</td>\n",
              "      <td>sci-fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The US military is running a test for a specia...</td>\n",
              "      <td>sci-fi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   genre\n",
              "0  The protagonist, space archaeologist Simon Wat...  sci-fi\n",
              "1  A woman (Lanell Cado) steps out of a shower an...  sci-fi\n",
              "2  The plot involves rogue elements of the commun...  sci-fi\n",
              "3  Set in contemporary England, the film follows ...  sci-fi\n",
              "4  The US military is running a test for a specia...  sci-fi"
            ]
          },
<<<<<<< HEAD
          "execution_count": 1027,
=======
          "execution_count": 43,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess train data\n",
        "data = preprocess_data(data)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecfcf52e",
      "metadata": {
        "id": "ecfcf52e"
      },
      "source": [
        "## Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1028,
      "id": "340dfb88",
      "metadata": {},
=======
      "execution_count": 44,
      "id": "c3968266",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c3968266"
      },
      "outputs": [],
      "source": [
        "len(data['genre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "blPLZCX3n4H3",
      "metadata": {
        "id": "blPLZCX3n4H3"
      },
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "outputs": [],
      "source": [
        "# Preprocess test data\n",
        "data_test = DataFrame({'text': X_test})\n",
<<<<<<< HEAD
        "X_test = preprocess_data(data_test)['text']\n",
=======
        "X_test = preprocess_data(data_test)['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6ecb944c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['models/label_encoder_0.plk']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data['genre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "ebb7e102",
      "metadata": {},
      "outputs": [],
      "source": [
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "y_train = data['genre']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecfcf52e",
      "metadata": {
        "id": "ecfcf52e"
      },
      "source": [
        "## Stopwords filtering"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1029,
=======
      "execution_count": 47,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "d70452bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = data['text']\n",
        "X_train_series = Series(X_train)\n",
        "\n",
        "# CountVectorizer to create a document-term matrix for stop words\n",
        "vectorizer = CountVectorizer(vocabulary=stop_words, binary=True)  # Count presence\n",
        "X = vectorizer.fit_transform(X_train_series)  # Transform the title-plot feature\n",
        "\n",
        "# Compute document frequencies\n",
        "doc_freq = X.sum(axis=0) / X.shape[0]  # Fraction of documents (observations) containing each stop word\n",
        "\n",
        "# Sort stop words by document frequency.\n",
        "stop_word_df = DataFrame({\n",
        "    'stop_word': vectorizer.get_feature_names_out(),\n",
        "    'doc_frequency': doc_freq.A1\n",
        "}).sort_values(by='doc_frequency', ascending=False)\n",
        "\n",
        "# Set a threshold for significant stop words\n",
        "def threshold_stop_word_df():\n",
        "  return stop_word_df[stop_word_df['doc_frequency'] >= threshold]\n",
        "\n",
        "stop_words_filtered = threshold_stop_word_df()['stop_word'].tolist()\n",
        "stop_words_filtered += label_encoder.classes_.tolist()"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1030,
=======
      "execution_count": 48,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "Ds8p9Rm8iMmO",
      "metadata": {
        "id": "Ds8p9Rm8iMmO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gleitao/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fi', 'sci'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Feature extraction function\n",
        "def extract_features():\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        ngram_range=ngram_range,\n",
        "        sublinear_tf=True,\n",
        "        norm='l2', stop_words=stop_words_filtered\n",
        "    )\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return X_train_vec, X_test_vec, vectorizer\n",
        "\n",
        "\n",
        "X_train_tfidf, X_test_tfidf, vectorizer = extract_features()"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1031,
=======
      "execution_count": 49,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "64f8b000",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "[\"models/plot/['from', 'director', 'title']_15_(1, 6)/vectorizer.plk\"]"
            ]
          },
          "execution_count": 1031,
=======
              "[\"models/['from', 'director']_30_(1, 5)/vectorizer.plk\"]"
            ]
          },
          "execution_count": 49,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save vectorizers\n",
        "dump(vectorizer, vectorizer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890dcfb3",
      "metadata": {
        "id": "890dcfb3"
      },
      "source": [
        "## Train SVM"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1032,
=======
      "execution_count": 50,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "Su-q0Jpnk_CW",
      "metadata": {
        "id": "Su-q0Jpnk_CW"
      },
      "outputs": [],
      "source": [
        "#kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1033,
=======
      "execution_count": 51,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "ZhUqV8chU2bA",
      "metadata": {
        "id": "ZhUqV8chU2bA"
      },
      "outputs": [],
      "source": [
        "def train_svm():\n",
        "    param_grid = {\n",
<<<<<<< HEAD
        "        'C': [1.0],\n",
        "        'kernel': ['sigmoid'],\n",
=======
        "        'C': [1],\n",
        "        'kernel': ['linear'],\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "        'class_weight': [None]\n",
        "    }\n",
        "    grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train_tfidf, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"Best SVM Parameters: {grid_search.best_params_}\")\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1034,
=======
      "execution_count": 52,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "6c285495",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Best SVM Parameters: {'C': 1.0, 'class_weight': None, 'kernel': 'sigmoid'}\n"
=======
            "Best SVM Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n"
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          ]
        }
      ],
      "source": [
        "# SVM training \n",
        "svm_tfidf = train_svm()"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1035,
=======
      "execution_count": 53,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "Bw5jnzGc4q-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Bw5jnzGc4q-P",
        "outputId": "314afa4b-692d-456e-e349-f5c68014fbca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "[\"models/plot/['from', 'director', 'title']_15_(1, 6)/svm.plk\"]"
            ]
          },
          "execution_count": 1035,
=======
              "['models/svm/svm_0.plk']"
            ]
          },
          "execution_count": 53,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save SVM models\n",
        "dump(svm_tfidf, svm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626081f0",
      "metadata": {},
      "source": [
        "## Train Multinomial NB with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1036,
=======
      "execution_count": 54,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "8b9a58fb",
      "metadata": {
        "id": "8b9a58fb"
      },
      "outputs": [],
      "source": [
        "# Train Multinomial NB with hyperparameter tuning\n",
        "def train_MultinomialNB():\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Smoothing parameter\n",
        "        'fit_prior': [True, False],           # Whether to learn class prior probabilities\n",
        "    }\n",
        "    \n",
        "    grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train_tfidf, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    _\n",
        "    print(f\"Best Multinomial NB Parameters: {grid_search.best_params_}\")\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1037,
=======
      "execution_count": 55,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "76241b6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Multinomial NB Parameters: {'alpha': 0.1, 'fit_prior': True}\n"
          ]
        }
      ],
      "source": [
        "# MultinomialNB training \n",
        "MultinomialNB_tfidf = train_MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1038,
=======
      "execution_count": 56,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "592f07e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "[\"models/plot/['from', 'director', 'title']_15_(1, 6)/mnb.plk\"]"
            ]
          },
          "execution_count": 1038,
=======
              "['models/mnb/mnb_0.plk']"
            ]
          },
          "execution_count": 56,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save MultinomialNB models\n",
        "dump(MultinomialNB_tfidf, mnb_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9725a0f3",
      "metadata": {
        "id": "9725a0f3"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1039,
=======
      "execution_count": 57,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "bccef53b",
      "metadata": {
        "id": "bccef53b"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def evaluate_model(model):\n",
        "    predictions = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    report = classification_report(y_test, predictions, target_names=label_encoder.classes_)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    return accuracy, cm, report"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1040,
=======
      "execution_count": 58,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "0c5a0f9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy SVM_tfidf: 70.56%\n",
            "Accuracy MultinomialNB_tfidf: 66.21%\n"
=======
            "Accuracy SVM_tfidf: 77.46%\n",
            "Accuracy MultinomialNB_tfidf: 72.85%\n"
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
          ]
        }
      ],
      "source": [
<<<<<<< HEAD
        "accuracy_svm, report_svm, cm_svm = evaluate_model(svm_tfidf)\n",
        "print(f\"Accuracy SVM_tfidf: {accuracy_svm * 100:.2f}%\")\n",
        "\n",
        "accuracy_mnb, report_mnb, cm_mnb = evaluate_model(MultinomialNB_tfidf)\n",
=======
        "# Evaluate SVM\n",
        "accuracy_svm, cm_svm, report_svm = evaluate_model(svm_tfidf)\n",
        "print(f\"Accuracy SVM_tfidf: {accuracy_svm * 100:.2f}%\")\n",
        "\n",
        "# Evaluate Multinomial Naive Bayes\n",
        "accuracy_mnb, cm_mnb, report_mnb = evaluate_model(MultinomialNB_tfidf)\n",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
        "print(f\"Accuracy MultinomialNB_tfidf: {accuracy_mnb * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
      "id": "91cb2de4",
=======
      "id": "4812e73d",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "metadata": {},
      "source": [
        "### Determine the best model"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1041,
      "id": "95db7236",
=======
      "execution_count": 345,
      "id": "14239151",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the best model\n",
        "if accuracy_svm >= accuracy_mnb:\n",
        "    best_model_path = svm_path\n",
        "    best_report = report_svm\n",
<<<<<<< HEAD
        "    report_path = f'{base}/report_svm.txt'\n",
        "    best_cm = cm_svm\n",
        "    cm_path = f'{base}/cm_svm.csv'  \n",
        "else:\n",
        "    best_model_path = mnb_path\n",
        "    best_report = report_mnb\n",
        "    report_path = f'{base}/report_mnb.txt'\n",
        "    best_cm = cm_mnb\n",
        "    cm_path = f'{base}/cm_mnb.csv'\n",
        "\n",
        "# Save classification report\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(str(best_report))\n",
        "\n",
        "# Save confusion matrix\n",
        "cm_df = DataFrame(best_cm, index=label_encoder.classes_, columns=label_encoder.classes_)"
=======
        "    best_cm = cm_svm\n",
        "else:\n",
        "    best_model_path = mnb_path\n",
        "    best_report = report_mnb\n",
        "    best_cm = cm_mnb\n",
        "\n",
        "# Save the report and confusion matrix\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(best_report)\n",
        "best_cm_df = DataFrame(best_cm, columns=label_encoder.classes_, index=label_encoder.classes_)\n",
        "best_cm_df.to_csv(cm_path)"
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3701949e",
      "metadata": {
        "id": "3701949e"
      },
      "source": [
        "## Load the model and vectorizer"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1042,
=======
      "execution_count": 59,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "343e0f65",
      "metadata": {
        "id": "343e0f65"
      },
      "outputs": [],
      "source": [
        "# Load a certain model\n",
        "def load_model(model_path, vectorizer_path, label_encoder_path):\n",
        "    model = load(model_path)\n",
        "    vectorizer = load(vectorizer_path)\n",
        "    label_encoder = load(label_encoder_path)\n",
        "    return model, vectorizer, label_encoder"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1043,
=======
      "execution_count": 60,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "a1b3c420",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model, vectorizer, label_encoder = load_model(best_model_path, vectorizer_path, label_encoder_path)"
      ]
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
=======
      "id": "543d67e6",
      "metadata": {},
      "source": [
        "## Perform cross-validation on the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "9c5abe2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation Accuracy for the best model: 77.76% (+/- 1.12%)\n"
          ]
        }
      ],
      "source": [
        "# Perform cross-validation\n",
        "# cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "# print(f\"Cross-validation Accuracy for the best model: {cv_scores.mean() * 100:.2f}% (+/- {cv_scores.std() * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "54d25897",
      "metadata": {
        "id": "54d25897"
      },
      "source": [
        "## Predict the genre for new movie plots"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1044,
=======
      "execution_count": 62,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "083e8e35",
      "metadata": {
        "id": "083e8e35"
      },
      "outputs": [],
      "source": [
        "# Predict the genre of the test data\n",
        "def predict_genre():\n",
        "    data = read_csv(test_path, sep='\\t', names=['title', 'from', 'director', 'plot'])\n",
        "    data = preprocess_data()\n",
        "    plots = data['combined_text']\n",
        "    plot_vectors = vectorizer.transform(plots)\n",
        "    predicted_genres_encoded = model.predict(plot_vectors)\n",
        "    predicted_genres = label_encoder.inverse_transform(predicted_genres_encoded)\n",
        "\n",
        "    # Save the results to a file\n",
        "    data['genre'] = predicted_genres\n",
        "    data[['title', 'from', 'director', 'plot', 'genre']].to_csv(output_path, sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1045,
=======
      "execution_count": 63,
>>>>>>> c25f105faefc528835b0ec279562d0b7e56a2321
      "id": "b729fda3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict_genre()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LN (.venv)",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
